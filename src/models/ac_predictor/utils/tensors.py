# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

import math
from logging import getLogger

import torch
from torch import Tensor

logger = getLogger(__name__)


# PyTorch now provides torch.nn.init.trunc_normal_ natively.
# This custom implementation is kept for backward compatibility
# but new code should use torch.nn.init.trunc_normal_ directly.


def _no_grad_trunc_normal_(
    tensor: Tensor,
    mean: float,
    std: float,
    a: float,
    b: float,
) -> Tensor:
    """Fill tensor with values from a truncated normal distribution.

    Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf

    Args:
        tensor: Tensor to fill in-place.
        mean: Mean of the normal distribution.
        std: Standard deviation of the normal distribution.
        a: Lower truncation bound.
        b: Upper truncation bound.

    Returns:
        The filled tensor.

    Raises:
        ValueError: If std is not positive.

    Note:
        This function modifies tensor in-place without gradient tracking.
    """
    if std <= 0:
        raise ValueError(f"std must be positive (got {std})")

    def norm_cdf(x: float) -> float:
        """Compute standard normal cumulative distribution function."""
        return (1.0 + math.erf(x / 2**0.5)) / 2.0

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        lower = norm_cdf((a - mean) / std)
        upper = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [lower, upper], then translate to
        # [2*lower-1, 2*upper-1].
        tensor.uniform_(2 * lower - 1, 2 * upper - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * 2**0.5)
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(
    tensor: Tensor,
    mean: float = 0.0,
    std: float = 1.0,
    a: float = -2.0,
    b: float = 2.0,
) -> Tensor:
    """Fill tensor with values from a truncated normal distribution.

    Args:
        tensor: Tensor to fill in-place.
        mean: Mean of the normal distribution. Defaults to 0.0.
        std: Standard deviation of the normal distribution. Defaults to 1.0.
        a: Lower truncation bound. Defaults to -2.0.
        b: Upper truncation bound. Defaults to 2.0.

    Returns:
        The filled tensor.

    Note:
        Consider using torch.nn.init.trunc_normal_ for new code.
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def repeat_interleave_batch(x: Tensor, batch_size: int, repeat: int) -> Tensor:
    """Repeat each batch element along the batch dimension.

    This function takes a batch of shape (batch_size * num_blocks, ...) and
    repeats each batch element `repeat` times, resulting in shape
    (batch_size * num_blocks * repeat, ...).

    Args:
        x: Input tensor of shape (batch_size * num_blocks, ...).
        batch_size: The batch size B (first dimension contains B * N elements).
        repeat: Number of times to repeat each batch element.

    Returns:
        Tensor with repeated batch elements.

    Examples:
        >>> x = torch.tensor([1, 2, 3, 4, 5, 6])  # B=2, N=3
        >>> repeat_interleave_batch(x, batch_size=2, repeat=3)
        tensor([1, 2, 1, 2, 1, 2, 3, 4, 3, 4, 3, 4, 5, 6, 5, 6, 5, 6])
    """
    if batch_size <= 0:
        raise ValueError(f"batch_size must be positive (got {batch_size})")
    if repeat <= 0:
        raise ValueError(f"repeat must be positive (got {repeat})")

    num_blocks = len(x) // batch_size
    if len(x) % batch_size != 0:
        raise ValueError(
            f"x length {len(x)} is not divisible by batch_size {batch_size}"
        )

    # Reshape to (num_blocks, batch_size, ...), repeat, and reshape back
    shape = x.shape
    x = x.reshape(num_blocks, batch_size, *shape[1:])
    x = x.repeat_interleave(repeat, dim=0)
    x = x.reshape(num_blocks * repeat * batch_size, *shape[1:])
    return x
