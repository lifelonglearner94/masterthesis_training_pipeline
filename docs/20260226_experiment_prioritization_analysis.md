# Analyse: Experiment-Priorisierung & Cross-Validation-Notwendigkeit

**Datum:** 2026-02-26
**Kontext:** Budget- und zeitkritisches Training auf vast.ai GPUs. Betreuer-Vorgabe war 4-Phasen-Ansatz, CL-Experimente wurden vorgezogen (Phase 3 vor Phase 2).

---

## Status der Experimente

| Config | Status | GPU-Zeit (geschÃ¤tzt) | Zweck |
|---|---|---|---|
| `cl_ac_hope.yaml` | âœ… Fertig | ~Stunden | CL-Ansatz 2 (Titan Memory + DGD) |
| `cl_lower_bound.yaml` | ğŸ”„ LÃ¤uft | ~Stunden | Performance-Floor (naives Finetuning) |
| `cl_upper_bound.yaml` | â³ Ausstehend | ~Stunden | Performance-Ceiling (Joint Training) |
| `cl_upper_bound_cross_validation.yaml` | â³ Ausstehend | **~10Ã— so lang** (10 Folds!) | DatenqualitÃ¤ts-Check |
| `cl_ac_vit.yaml` | â³ Ausstehend | ~Stunden | CL-Ansatz 1 (TTA) |

---

## Frage 1: Kann `cl_ac_vit.yaml` (TTA) erstmal weggelassen werden?

### Empfehlung: **Ja, kann erstmal weggelassen werden.**

**BegrÃ¼ndung:**

Die **minimale vollstÃ¤ndige CL-Story** fÃ¼r die Thesis braucht genau drei Dinge:

1. **Lower Bound** (`cl_lower_bound`) â€” Performance-Floor: "So schlecht ist naives Finetuning"
2. **Upper Bound** (`cl_upper_bound`) â€” Performance-Ceiling: "So gut geht es maximal mit allen Daten"
3. **Ein CL-Ansatz** (`cl_ac_hope`) â€” "Meine Methode liegt zwischen Floor und Ceiling"

Das AC-ViT + TTA Experiment ist ein **zweiter CL-Ansatz** â€” nice to have fÃ¼r einen Methodenvergleich, aber nicht essentiell fÃ¼r die Kernaussage. Die AC-HOPE-Ergebnisse sind bereits stark (StreamForgetting = 0.042), das reicht als CL-Demonstration.

**PrioritÃ¤t:** Niedrig. Erst nach Lower Bound + Upper Bound, falls Budget Ã¼brig ist.

---

## Frage 2: Kann die Cross-Validation weggelassen werden?

### Empfehlung: **Ja, mit EinschrÃ¤nkung â€” die AC-HOPE-Ergebnisse liefern bereits indirekte Evidenz.**

### Was der Betreuer eigentlich prÃ¼fen wollte

> "Man trainiert ein Modell auf alle Daten gleichzeitig (Joint-Training). Man lÃ¤sst zufÃ¤llig Daten weg und testet via n-facher Kreuzvalidierung, ob das Modell diese prÃ¤dizieren/rekonstruieren kann."

Der Zweck ist: **Sicherstellen, dass die Tasks genÃ¼gend gute, reprÃ¤sentative Samples enthalten**, bevor man teure CL-Experimente startet. Es soll verhindern, dass man CL-Metriken auf Daten berechnet, die das Modell grundsÃ¤tzlich gar nicht lernen kann.

### Warum die AC-HOPE-Ergebnisse diese Frage bereits beantworten

Die `cl_ac_hope`-Ergebnisse zeigen **indirekt aber eindeutig**, dass die Daten ausreichend lernbar sind:

| Evidenz | Wert | Was es beweist |
|---|---|---|
| Base Training Val-Loss | ~0.326 | Modell kann Base-Daten lernen |
| Task 1â€“5 Pure Retrieval Loss | 0.327 â€“ 0.393 | Alle 5 Task-Partitionen sind lernbar |
| Backward Transfer nach Task 1 | -0.212 (negativ = Verbesserung!) | Task-Daten enthalten transferierbares Wissen |
| Losses Ã¼ber Tasks hinweg konsistent | 0.32 â€“ 0.39 Range | Kein Task ist ein AusreiÃŸer / unlernbar |
| StreamForgetting final | 0.042 | Stabile ReprÃ¤sentationen Ã¼ber alle Tasks |

**Kernargument:** Wenn ein Modell im **hÃ¤rtesten Setting** (sequenzielles CL mit frozen inner-loop bei Eval) alle Tasks auf 0.32â€“0.39 Loss bringt und dabei kaum vergisst, dann sind die Tasks offensichtlich lernbar und reprÃ¤sentativ. Ein Joint-Training CV-Check wÃ¼rde **zwangslÃ¤ufig gleich gute oder bessere Ergebnisse zeigen**, da Joint Training strikt einfacher ist als sequenzielles CL.

### Das Upper Bound Experiment deckt den CV-Check teilweise mit ab

`cl_upper_bound.yaml` trainiert auf **allen 10.000 Clips gleichzeitig** und evaluiert dann per Task-Partition. Wenn das Upper-Bound-Modell auf allen Task-Partitionen gute Losses zeigt, ist das **funktional Ã¤quivalent** zum CV-Check â€” es beweist, dass das Modell die Daten aller Tasks gleichzeitig lernen kann.

Der einzige Unterschied: CV mit 10 Folds gibt dir zusÃ¤tzlich Varianz-SchÃ¤tzungen (Std Ã¼ber Folds). Aber fÃ¼r den Zweck "sind die Daten lernbar?" reicht ein einzelnes Joint Training.

### Risiko-Bewertung

| Szenario | Risiko | Konsequenz |
|---|---|---|
| Betreuer fragt nach CV-Ergebnissen | **Mittel** | Du kannst argumentieren: "Upper Bound + AC-HOPE-Ergebnisse zeigen Lernbarkeit. CV hÃ¤tte redundante Information geliefert." |
| Reviewer fragt nach Datenvalidierung | **Niedrig** | Upper Bound IS die Validierung. Paper-Reviewer erwarten keine separate CV vor CL. |
| Ein Task wÃ¤re nicht lernbar gewesen | **Bereits widerlegt** | AC-HOPE hat alle Tasks erfolgreich gelernt. |

---

## Empfohlene Reihenfolge (Budget-optimiert)

```
1. âœ… cl_ac_hope          â€” FERTIG
2. ğŸ”„ cl_lower_bound      â€” LÃ„UFT â†’ abwarten
3. â­ cl_upper_bound       â€” ALS NÃ„CHSTES (essentiell + ersetzt teilweise CV)
4. â“ cl_ac_vit            â€” NUR falls Budget Ã¼brig
5. â“ cl_cross_validation  â€” NUR falls Betreuer explizit darauf besteht
```

### Warum Upper Bound jetzt PrioritÃ¤t hat

- Ohne Upper Bound fehlt die **obere Referenzlinie** â€” du kannst nicht zeigen, wo AC-HOPE relativ zum Optimum liegt
- Upper Bound trainiert **einmal** auf allen Daten (40 Epochs, 10k Clips) â€” deutlich billiger als 10-Fold CV (10Ã— Training!)
- Upper Bound + Lower Bound + AC-HOPE = **vollstÃ¤ndige CL-Evaluation** mit allen nÃ¶tigen Referenzpunkten

---

## Zusammenfassung

| Frage | Antwort |
|---|---|
| `cl_ac_vit` weglassen? | **Ja.** Nicht essentiell fÃ¼r die Kernaussage. |
| Cross-Validation weglassen? | **Ja, vertretbar.** AC-HOPE + Upper Bound liefern die gleiche Evidenz. |
| Was ist jetzt essentiell? | **Lower Bound (lÃ¤uft) + Upper Bound (als nÃ¤chstes starten).** |
| Dem Betreuer kommunizieren? | Ja â€” proaktiv erklÃ¤ren, dass Upper Bound + AC-HOPE-Ergebnisse die Lernbarkeit der Tasks bereits belegen, und fragen ob er trotzdem auf CV besteht. |
