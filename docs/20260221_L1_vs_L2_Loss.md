## Wissenschaftliches Protokoll: Evaluation und Auswahl der Loss-Funktion für V-JEPA 2 Feature Maps

**Titel:** Begründung des Wechsels zum L2 Loss beim Training auf V-JEPA 2 Repräsentationen im Kontext von Physik-Simulationen

**1. Hintergrund und Kontext**
Im aktuellen Experiment wird ein Deep-Learning-Modell trainiert, das auf den latenten Feature Maps des V-JEPA 2 Encoders operiert. Im Gegensatz zur originalen Anwendung des V-JEPA 2 Papers, welche sich auf reale Robotik-Kameradaten (Droid-Datensatz) konzentriert, findet das vorliegende Experiment in einer reinen Physik-Simulation statt. Es stellte sich die methodische Frage, ob die im Originalpaper verwendete L1-Fehlerfunktion (Mean Absolute Error) beibehalten oder auf eine L2-Fehlerfunktion (Mean Squared Error) gewechselt werden sollte.

**2. Theoretische Grundlage & Methodik des Originalpapers**

* **Dimensionalität:** Die Feature Maps des verwendeten V-JEPA 2 ViT-g Encoders sind mit einer Form von 16x16x1408 extrem hochdimensional.
* **L1-Fokus der Autoren:** Das Originalpaper nutzt durchgehend die L1-Norm. Dies gilt für den Teacher-Forcing Loss zur Vorhersage des nächsten Zeitschritts, den Rollout Loss über mehrere Schritte sowie für die Energiefunktion während der Planung von Handlungssequenzen.
* **Eigenschaften von L1 vs. L2:** L1 skaliert Fehler linear und verhindert in hochdimensionalen Räumen, dass einzelne Abweichungen (Outliers) den Gradienten überproportional dominieren. L2 quadriert Fehler, was zu einer sanfteren Konvergenz nahe dem Minimum führt, das Modell aber empfindlicher für Ausreißer macht.

**3. Domänentransfer: Robotik vs. Physik-Simulation**

* In der **Robotik** (Originalpaper) ist die Robustheit des L1 Loss entscheidend, um das Rauschen und die Unberechenbarkeit realer Sensordaten (Kamera-Artefakte, Lichtverhältnisse, unvorhersehbare physikalische Interaktionen) auszugleichen.
* In einer **Physik-Simulation** (vorliegendes Experiment) entfällt das Sensorrauschen vollständig. Da physikalische Vorhersagen oft höchste Präzision erfordern und Fehler in der Trajektorie sich über die Zeit aufsummieren (Drift), bietet der L2 Loss hier den entscheidenden Vorteil, Abweichungen hart zu bestrafen und gleichzeitig glatte Konvergenzen zu erzwingen.

**4. Finale Entscheidung und Rationale**
**Beschluss:** Das Training wird von der L1-Norm auf den **L2 Loss (Mean Squared Error)** umgestellt.

**Begründung:** Da das experimentelle Continual Learning Curriculum methodisch neu aufgesetzt wurde (Akkumulations-Strategie für Task 1 bis 5) und ein vollständiges Neutraining der Baseline ohnehin unumgänglich ist, entfällt das bisherige pragmatische Sunk-Cost-Argument (Ressourceneffizienz bestehender Runs). Diese Gelegenheit wird genutzt, um die Fehlerfunktion optimal an die Domäne der simulierten Physik anzupassen:

1. **Domänenspezifische Präzision (Fehlende Ausreißer):** In der rauschfreien, deterministischen Kubric-Simulation gibt es keine unvorhersehbaren Sensor-Outliers wie in echten Roboter-Daten. Daher ist die extreme Robustheit von L1 gegenüber Ausreißern hier nicht erforderlich. Stattdessen bestraft L2 kleine Fehler in der latenten Trajektorie quadratisch und erzwingt so eine striktere, kausale Einhaltung der simulierten Dynamiken (z.B. Beschleunigung und Abstoppen).
2. **Glatte Konvergenz und Vermeidung von Gradientenzittern:** L1 liefert auch extrem nahe am physikalischen Optimum einen konstanten Gradienten. Im hochdimensionalen Latent Space (16x16x1408) kann dies am Ende des Trainings zu einem kontinuierlichen "Zittern" (Oszillation) der Vorhersagen führen. L2 sorgt durch kontinuierlich abflachende Gradienten nahe der Null-Marke für wesentlich glattere, stabilere Latent-Trajektorien. Dies ist für die Vorhersage von reibungsfreiem Gleiten oder dem kompletten Stillstand eines Objekts essenziell.
3. **Synergie mit dem Continual Learning Setup:** Das neue 5-Task-Setup verlangt absolute Präzision bei der Evaluierung von Transfer-Metriken (Forward/Backward Transfer). Eine mathematisch sauber konvergierte Baseline durch den L2 Loss stellt sicher, dass gemessenes "Forgetting" tatsächlich auf architektonische Limits und nicht auf numerische Instabilitäten im Minimum zurückzuführen ist.
