# HOPE vs ViT Research

Gold standard research repository comparing HOPE and Vision Transformers (ViT).

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- [uv](https://github.com/astral-sh/uv) - Fast Python package manager

### Installation

```bash
# Install dependencies
make install
# or directly with uv
uv sync
```

### Training

```bash
# Train with default config
make train

# Train with specific experiment
uv run src/train.py experiment=example

# Train with custom overrides
uv run src/train.py model=hope trainer=gpu seed=123
```

### AC Predictor Model

The repository includes an Action-Conditioned Vision Transformer Predictor (from V-JEPA2) for training on pre-computed encoder features.

```bash
# Train AC Predictor with pre-computed features
uv run src/train.py model=ac_predictor data=precomputed_features

# With GPU and custom batch size
uv run src/train.py model=ac_predictor data=precomputed_features trainer=gpu data.batch_size=16
```

**Expected data format** (`.npy` files):
```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ episode_0000/
â”‚   â”‚   â”œâ”€â”€ features.npy   # [T+1, N, D] encoder features
â”‚   â”‚   â”œâ”€â”€ actions.npy    # [T, 7] end-effector changes
â”‚   â”‚   â””â”€â”€ states.npy     # [T, 7] end-effector states
â”‚   â””â”€â”€ ...
â””â”€â”€ val/
    â””â”€â”€ ...
```

### Testing

```bash
make test
```

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ configs/                 # Hydra configuration files
â”‚   â”œâ”€â”€ config.yaml         # Main config entry point
â”‚   â”œâ”€â”€ callbacks/          # Callback configurations
â”‚   â”œâ”€â”€ data/               # DataModule configurations
â”‚   â”œâ”€â”€ experiment/         # Full experiment configs
â”‚   â”œâ”€â”€ hparams_search/     # Hyperparameter search configs
â”‚   â”œâ”€â”€ logger/             # Logger configurations
â”‚   â”œâ”€â”€ model/              # Model configurations
â”‚   â”œâ”€â”€ paths/              # Path configurations
â”‚   â””â”€â”€ trainer/            # Trainer configurations
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ callbacks/          # Custom callbacks
â”‚   â”œâ”€â”€ datamodules/        # PyTorch Lightning DataModules
â”‚   â”œâ”€â”€ models/             # Model implementations
â”‚   â””â”€â”€ utils/              # Utility functions
â”œâ”€â”€ tests/                  # Unit tests
â”œâ”€â”€ pyproject.toml          # Project dependencies (PEP 621)
â”œâ”€â”€ uv.lock                 # Locked dependencies
â”œâ”€â”€ Dockerfile              # Container definition
â””â”€â”€ Makefile                # Workflow automation
```

## ğŸ”§ Configuration

This project uses [Hydra](https://hydra.cc/) for configuration management. The main config file is `configs/config.yaml`.

### Running Experiments

```bash
# Override single parameters
uv run src/train.py trainer.max_epochs=100

# Use predefined experiments
uv run src/train.py experiment=example

# Multi-run with different seeds
uv run src/train.py --multirun seed=42,123,456
```

### Hyperparameter Search

```bash
uv run src/train.py --multirun hparams_search=optuna
```

## ğŸ³ Docker

```bash
# Build image
docker build -t hope-vit-research .

# Run training
docker run --gpus all hope-vit-research python src/train.py
```

## ğŸ“Š Logging

Experiments are logged to [Weights & Biases](https://wandb.ai/). Set your API key:

```bash
cp .env.example .env
# Edit .env with your WANDB_API_KEY
```

## ğŸ“ License

MIT
