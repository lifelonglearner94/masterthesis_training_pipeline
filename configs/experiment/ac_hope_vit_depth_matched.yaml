# @package _global_
# =============================================================================
# Experiment 2: AC-HOPE-ViT — Depth-Matched (12 layers, ~83M)
# =============================================================================
# Use as: uv run src/train.py experiment=ac_hope_vit_depth_matched paths.data_dir=/path/to/clips
#
# COMPARISON DESIGN:
#   Baseline (vjepa2_ac):  depth=24, standard Transformer    → 43.38M params
#   This experiment:       depth=12, HOPE (Titan+CMS)        → 83.29M params
#   Difference: +92% more params (depth-controlled, not parameter-controlled)
#
# SCIENTIFIC RATIONALE:
#   This is a depth-controlled comparison. Both models have meaningful depth
#   for sequential feature processing (12 vs 24 layers). The parameter
#   asymmetry (83M vs 43M) is the acknowledged tradeoff — it's inherent to
#   the HOPE architecture which uses 5 Titan memories per block.
#
#   Together with the parameter-matched experiment (ac_hope_vit_param_matched,
#   depth=6, 42M), these two experiments bracket the true effect:
#     - Param-matched (d=6):  controls for capacity, confounds depth
#     - Depth-matched (d=12): controls for depth, confounds capacity
#
#   If HOPE outperforms in BOTH experiments, the evidence is strong.
#   If only in one, the confound explains the difference.
#
# KEY SETTINGS:
#   - depth: 12                    (half of baseline, but still deep enough)
#   - titan_hidden_multiplier: 2   (moderate Titan MLP size)
#   - CMS levels: 3 × 4.0         (full multi-timescale hierarchy)
#
# TEMPORAL DIMENSIONS:
#   - Original video: 16 frames
#   - Precomputed features: 8 timesteps (16 frames / tubelet_size=2)
#   - Available prediction steps: 7 (num_timesteps - 1 = 8 - 1)
#   - T_teacher/jump_k must be <= 7
# =============================================================================

defaults:
  - override /data: precomputed_features
  - override /model: ac_hope_vit
  - override /trainer: default
  - override /callbacks: default
  - override /logger: wandb

callbacks:
  verbose_logging:
    _target_: src.callbacks.verbose_logging.VerboseLoggingCallback
    log_memory: False
    log_data: False
    log_forward_pass: False
    log_backward_pass: False
    log_gradients: False
    log_weights: False
    log_every_n_batches: 16
    max_layers_to_log: 30

# =============================================================================
# EXPERIMENT METADATA
# =============================================================================
task_name: "ac_hope_vit_depth_matched"
tags: ["hope", "titan", "cms", "depth_matched", "depth_12", "robotics"]
seed: 42
train: True
test: False

# =============================================================================
# ⚠️  MEMORY-CRITICAL SETTINGS
# =============================================================================
# With depth=12 and titan_hm=2, this model is ~2× the baseline in parameters
# and ~2-3× in memory due to Titan inner-loop gradients.
#
# If you run out of memory, reduce in order:
#   1. batch_size: 32 → 16 → 8 → 4
#   2. num_workers: 8 → 4 → 0
#   3. cms_level_specs: remove "slow" level
#   4. use_activation_checkpointing: true (already enabled)

data:
  batch_size: 32                   # ⚠️ Lower than baseline (12 HOPE blocks)
  num_workers: 8
  pin_memory: true

model:
  # --- Temporal / loss settings (adapted for HOPE bi-level optimization) ---
  T_teacher: 7
  jump_k: 3                     # Number of candidate jump targets
  use_activation_checkpointing: true  # ⚠️ Needed for 12-layer HOPE

  # --- Action dimension (dataset-specific) ---
  action_embed_dim: 2              # 2D actions for this dataset

  # --- HOPE architecture (depth-matched) ---
  depth: 12                        # Depth-matched: meaningful sequential depth
  titan_hidden_multiplier: 2       # Titan MLP hidden = 384×2 = 768
  use_rope: true                   # 3D RoPE enabled (Criticism §2: toggleable)
  titan_grad_clip_inner: 1.0       # Inner-loop gradient clipping
  chunk_size: 1                    # 0 = safe default; 1 = also trains M_eta/M_alpha (needs ~2-3× VRAM)
  titan_detach_interval: 4         # Detach every step (12-layer needs aggressive bounding)
  surprise_threshold: 0.0          # 0 = always update memories

  # CMS levels — full multi-timescale hierarchy
  cms_level_specs:
    - name: "fast"
      update_period: 1
      hidden_multiplier: 4.0
      warmup_steps: 0
    - name: "medium"
      update_period: 4
      hidden_multiplier: 4.0
      warmup_steps: 0
    - name: "slow"
      update_period: 16
      hidden_multiplier: 4.0
      warmup_steps: 0

  # --- Diagnostics (Criticism §1) ---
  log_hope_diagnostics: true

  # --- Loss weights ---
  loss_weight_teacher: 1.0
  loss_weight_jump: 1.0

  # --- Loss function (see docs/20260221_L1_vs_L2_Loss.md) ---
  loss_type: "l2"
  # huber_delta: 1.0  # Uncomment when loss_type="huber"

  # --- Curriculum Learning Schedule (adapted for HOPE: delayed phases) ---
  # HOPE's meta-learning needs longer teacher-forcing stabilization.
  # Never go below 0.5 — teacher signal is the primary stable gradient for meta-learning.
  curriculum_schedule:
    - epoch: 0
      loss_weight_teacher: 1.0
    - epoch: 15
      loss_weight_teacher: 0.7
    - epoch: 25
      loss_weight_teacher: 0.5

  # --- Optimizer (adapted for HOPE bi-level optimization) ---
  # See docs/HOPE_training_settings_evaluation.md for scientific rationale.
  learning_rate: 1.5e-4             # ↓ from 4.25e-4 (bi-level needs lower outer LR)
  weight_decay: 0.04               # Applied to CMS + projections
  betas: [0.9, 0.999]

  # Per-group LR scaling
  titan_lr_scale: 0.2              # ↓ from 0.5 — Titan LR = 1.5e-4 × 0.2 = 3e-5
  cms_lr_scale: 1.0                # CMS LR = 1.5e-4 × 1.0 = 1.5e-4
  titan_weight_decay: 0.005        # ↓ from 0.04 — avoid double regularization with inner-loop α
  aux_loss_weight: 0.1             # Weight for M_k/M_v auxiliary retrieval-quality loss

  # --- LR schedule (longer warmup for meta-learning stabilization) ---
  use_iteration_scheduler: true
  warmup_pct: 0.15                 # ↑ from 0.085 (HOPE needs longer warmup)
  constant_pct: 0.75               # Adjusted to sum to 1.0
  decay_pct: 0.10
  warmup_start_lr: 3.0e-5          # ↓ proportional to new peak LR

# =============================================================================
# TRAINER SETTINGS (adapted for HOPE bi-level optimization)
# =============================================================================
trainer:
  max_epochs: 30                   # ↑ from 30 (meta-learning converges slower)
  gradient_clip_val: 3.0           # ↑ from 1.01 (allow meta-gradient signal through)
  gradient_clip_algorithm: norm
