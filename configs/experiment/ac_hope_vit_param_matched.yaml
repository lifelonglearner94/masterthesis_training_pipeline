# @package _global_
# =============================================================================
# Experiment 1: AC-HOPE-ViT — Parameter-Matched (~42M vs 43M baseline)
# =============================================================================
# Use as: uv run src/train.py experiment=ac_hope_vit_param_matched paths.data_dir=/path/to/clips
#
# COMPARISON DESIGN:
#   Baseline (vjepa2_ac):  depth=24, standard Transformer    → 43.38M params
#   This experiment:       depth=6,  HOPE (Titan+CMS)        → 42.04M params
#   Difference: -3.1% (parameter-matched within noise)
#
# SCIENTIFIC NOTE:
#   This is a parameter-controlled comparison. The depth asymmetry (6 vs 24
#   layers) is an acknowledged limitation — each HOPE block is ~4× heavier
#   than a standard Transformer block due to 5 Titan memories + 3 CMS MLPs.
#   See the depth-matched experiment (ac_hope_vit_depth_matched) for the
#   complementary comparison that controls for depth instead.
#
# KEY CHANGES vs full config:
#   - depth:                  24 → 6   (main parameter lever)
#   - titan_hidden_multiplier: 4 → 2   (halves Titan MLP: 384×2=768)
#   - CMS levels:             3 × 4.0  (kept — preserves multi-timescale design)
#
# TEMPORAL DIMENSIONS:
#   - Original video: 16 frames
#   - Precomputed features: 8 timesteps (16 frames / tubelet_size=2)
#   - Available prediction steps: 7 (num_timesteps - 1 = 8 - 1)
#   - T_teacher/T_rollout must be <= 7
# =============================================================================

defaults:
  - override /data: precomputed_features
  - override /model: ac_hope_vit
  - override /trainer: default
  - override /callbacks: default
  - override /logger: wandb

callbacks:
  verbose_logging:
    _target_: src.callbacks.verbose_logging.VerboseLoggingCallback
    log_memory: False
    log_data: False
    log_forward_pass: False
    log_backward_pass: False
    log_gradients: False
    log_weights: False
    log_every_n_batches: 16
    max_layers_to_log: 30

# =============================================================================
# EXPERIMENT METADATA
# =============================================================================
task_name: "ac_hope_vit_param_matched"
tags: ["hope", "titan", "cms", "param_matched", "depth_6", "robotics"]
seed: 42
train: True
test: False

# =============================================================================
# ⚠️  MEMORY-CRITICAL SETTINGS
# =============================================================================
# With depth=6 and titan_hm=2, memory usage is comparable to the baseline.
# Batch size can be higher than the full 24-layer HOPE config.
#
# If you run out of memory, reduce in order:
#   1. batch_size: 64 → 32 → 16 → 8
#   2. num_workers: 8 → 4 → 0
#   3. use_activation_checkpointing: true

data:
  batch_size: 64                   # Comparable to baseline (6 layers = lighter)
  num_workers: 8
  pin_memory: true

model:
  # --- Temporal / loss settings (adapted for HOPE bi-level optimization) ---
  T_teacher: 7
  T_rollout: 7                     # ↓ from 7: memory corruption feedback loop over long rollouts
  context_frames: 1
  use_activation_checkpointing: false   # HOPE DGD updates mutate state during forward — incompatible with checkpoint recomputation

  # --- Action dimension (dataset-specific) ---
  action_embed_dim: 2              # 2D actions for this dataset

  # --- HOPE architecture (scaled for ~42M params) ---
  depth: 6                         # ⚠️ Reduced from 24 (parameter matching)
  titan_hidden_multiplier: 2       # ⚠️ Reduced from 4 (384×2=768 hidden)
  use_rope: true                   # 3D RoPE enabled (Criticism §2: toggleable)
  titan_grad_clip_inner: 1.0       # Inner-loop gradient clipping
  chunk_size: 1                    # 0 = safe default; 1 = also trains M_eta/M_alpha (needs ~2-3× VRAM)
  titan_detach_interval: 1         # Detach every step (MPS/consumer safe); set 4+ on datacenter GPU
  surprise_threshold: 0.0          # 0 = always update memories

  # CMS levels — full multi-timescale hierarchy preserved
  cms_level_specs:
    - name: "fast"
      update_period: 1
      hidden_multiplier: 4.0
      warmup_steps: 0
    - name: "medium"
      update_period: 4
      hidden_multiplier: 4.0
      warmup_steps: 0
    - name: "slow"
      update_period: 16
      hidden_multiplier: 4.0
      warmup_steps: 0

  # --- Diagnostics (Criticism §1) ---
  log_hope_diagnostics: true

  # --- Loss weights ---
  loss_weight_teacher: 1.0
  loss_weight_rollout: 1.0

  # --- Curriculum Learning Schedule (adapted for HOPE: delayed phases) ---
  # HOPE's meta-learning needs longer teacher-forcing stabilization.
  # Never go below 0.5 — teacher signal is the primary stable gradient for meta-learning.
  curriculum_schedule:
    - epoch: 0
      loss_weight_teacher: 1.0
    - epoch: 15
      loss_weight_teacher: 0.7
    - epoch: 25
      loss_weight_teacher: 0.5

  # --- Optimizer (adapted for HOPE bi-level optimization) ---
  # See docs/HOPE_training_settings_evaluation.md for scientific rationale.
  learning_rate: 1.5e-4             # ↓ from 4.25e-4 (bi-level needs lower outer LR)
  weight_decay: 0.04               # Applied to CMS + projections
  betas: [0.9, 0.999]

  # Per-group LR scaling
  titan_lr_scale: 0.2              # ↓ from 0.5 — Titan LR = 1.5e-4 × 0.2 = 3e-5
  cms_lr_scale: 1.0                # CMS LR = 1.5e-4 × 1.0 = 1.5e-4
  titan_weight_decay: 0.005        # ↓ from 0.04 — avoid double regularization with inner-loop α
  aux_loss_weight: 0.1             # Weight for M_k/M_v auxiliary retrieval-quality loss

  # --- LR schedule (longer warmup for meta-learning stabilization) ---
  use_iteration_scheduler: true
  warmup_pct: 0.15                 # ↑ from 0.085 (HOPE needs longer warmup)
  constant_pct: 0.75               # Adjusted to sum to 1.0
  decay_pct: 0.10
  warmup_start_lr: 3.0e-5          # ↓ proportional to new peak LR

# =============================================================================
# TRAINER SETTINGS (adapted for HOPE bi-level optimization)
# =============================================================================
trainer:
  max_epochs: 60                   # ↑ from 30 (meta-learning converges slower)
  precision: 32                    # HOPE inner-loop needs full precision (MPS fp16 support limited)
  gradient_clip_val: 3.0           # ↑ from 1.01 (allow meta-gradient signal through)
  gradient_clip_algorithm: norm
