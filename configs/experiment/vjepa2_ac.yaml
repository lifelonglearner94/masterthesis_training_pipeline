# @package _global_
# =============================================================================
# V-JEPA2 AC Predictor - SINGLE SOURCE OF TRUTH
# =============================================================================
# Use as: uv run src/train.py experiment=vjepa2_ac paths.data_dir=/path/to/clips
#
# This file consolidates ALL important settings. Edit values here, not in
# configs/model/ac_predictor.yaml or configs/data/precomputed_features.yaml.
#
# TEMPORAL DIMENSIONS:
#   - Original video: 16 frames
#   - Precomputed features: 8 timesteps (16 frames / tubelet_size=2)
#   - Available prediction steps: 7 (num_timesteps - 1 = 8 - 1)
#   - T_teacher/jump_k must be <= 7
# =============================================================================

defaults:
  - override /data: precomputed_features
  - override /model: ac_predictor
  - override /trainer: default
  - override /callbacks: default
  - override /logger: wandb

callbacks:
  verbose_logging:
    _target_: src.callbacks.verbose_logging.VerboseLoggingCallback
    log_memory: False
    log_data: False
    log_forward_pass: False
    log_backward_pass: False
    log_gradients: False
    log_weights: False
    log_every_n_batches: 16  # Log every single batch
    max_layers_to_log: 30   # Log up to 30 layers with hooks

# =============================================================================
# EXPERIMENT METADATA
# =============================================================================
task_name: "vjepa2_ac_predictor"
tags: ["vjepa2", "ac_predictor", "robotics"]
seed: 42
train: True
test: False

# =============================================================================
# ⚠️  MEMORY-CRITICAL SETTINGS (tune these for your 32GB RAM)
# =============================================================================
# If you run out of memory, reduce these values in order:
#   1. batch_size: 8 → 4 → 2
#   2. T_teacher: 5 → 3 → 2
#   3. num_workers: 2 → 0
#   4. use_activation_checkpointing: true (trades speed for ~50% less memory)

data:
  batch_size: 64                   # ⚠️ REDUCED from 32 (saves ~4x memory)
  num_workers: 8                   # ⚠️ REDUCED from 4 (each worker duplicates data)
  pin_memory: true

model:
  # --- Memory settings ---
  T_teacher: 7                     # ⚠️ REDUCED from 7 (max for 8 timesteps)
  jump_k: 3                     # Number of candidate jump targets (τ from last k frames)
  use_activation_checkpointing: true  # ⚠️ ENABLED (trades compute for ~50% memory)

  # --- Model architecture (keep defaults from ac_predictor.yaml) ---
  # Uncomment to override:
  # num_timesteps: 8  # Encoded timesteps (default, matches precomputed features)
  # embed_dim: 1024
  # predictor_embed_dim: 384
  # depth: 24
  # num_heads: 16
  # mlp_ratio: 4.0

  # --- Action dimension (dataset-specific) ---
  action_embed_dim: 2              # 2D actions for this dataset

  # --- Loss weights ---
  loss_weight_teacher: 1.0
  loss_weight_jump: 1.0

  # --- Loss function (see docs/20260221_L1_vs_L2_Loss.md) ---
  # "l1" = MAE (paper default, robust to outliers)
  # "l2" = MSE (better for noise-free physics sim, smooth convergence)
  # "huber" = Huber loss (compromise: L2 near zero, L1 for large errors)
  loss_type: "l2"
  # huber_delta: 1.0  # Uncomment when loss_type="huber"

  # --- Curriculum Learning Schedule ---
  # Dynamically adjusts loss_weight_teacher during training.
  # jump_k is fixed at init. Curriculum adjusts loss weights.
  # Each phase activates at the specified epoch and remains active until the next phase.
  # Parameters not specified in a phase retain their previous values.
  #
  # Recommended progression:
  #   Phase 1: Stabilization - high teacher weight
  #   Phase 2: Transition - reduce teacher weight
  #   Phase 3: Jump-dominant - model learns long-horizon dynamics
  #
  # Set to null to disable curriculum learning and use fixed values above.
  curriculum_schedule:
    - epoch: 0
      loss_weight_teacher: 1.0
    - epoch: 7
      loss_weight_teacher: 0.7
    - epoch: 11
      loss_weight_teacher: 0.3

  # --- Optimizer (V-JEPA2 paper settings) ---
  learning_rate: 4.25e-4           # Peak LR
  weight_decay: 0.04
  betas: [0.9, 0.999]

  # --- LR schedule (iteration-based, V-JEPA2 paper) ---
  # Percentages of total training iterations (must sum to 1.0)
  use_iteration_scheduler: true
  warmup_pct: 0.085       # ~8.5% warmup
  constant_pct: 0.83      # ~83% constant LR
  decay_pct: 0.085        # ~8.5% cosine decay
  warmup_start_lr: 7.5e-5

# =============================================================================
# TRAINER SETTINGS
# =============================================================================
trainer:
  max_epochs: 30
  gradient_clip_val: 1.01
  gradient_clip_algorithm: norm
