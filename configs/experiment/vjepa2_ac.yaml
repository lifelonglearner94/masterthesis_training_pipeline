# @package _global_
# =============================================================================
# V-JEPA2 AC Predictor - SINGLE SOURCE OF TRUTH
# =============================================================================
# Use as: uv run src/train.py experiment=vjepa2_ac paths.data_dir=/path/to/clips
#
# This file consolidates ALL important settings. Edit values here, not in
# configs/model/ac_predictor.yaml or configs/data/precomputed_features.yaml.
#
# TEMPORAL DIMENSIONS:
#   - Original video: 16 frames
#   - Precomputed features: 8 timesteps (16 frames / tubelet_size=2)
#   - Available prediction steps: 7 (num_timesteps - 1 = 8 - 1)
#   - T_teacher/T_rollout must be <= 7
# =============================================================================

defaults:
  - override /data: precomputed_features
  - override /model: ac_predictor
  - override /trainer: default
  - override /callbacks: default
  - override /logger: wandb

callbacks:
  verbose_logging:
    _target_: src.callbacks.verbose_logging.VerboseLoggingCallback
    log_memory: True
    log_data: True
    log_forward_pass: True
    log_backward_pass: True
    log_gradients: True
    log_weights: True
    log_every_n_batches: 16  # Log every single batch
    max_layers_to_log: 30   # Log up to 30 layers with hooks

# =============================================================================
# EXPERIMENT METADATA
# =============================================================================
task_name: "vjepa2_ac_predictor"
tags: ["vjepa2", "ac_predictor", "robotics"]
seed: 42
train: True
test: True

# =============================================================================
# ⚠️  MEMORY-CRITICAL SETTINGS (tune these for your 32GB RAM)
# =============================================================================
# If you run out of memory, reduce these values in order:
#   1. batch_size: 8 → 4 → 2
#   2. T_teacher: 5 → 3 → 2
#   3. num_workers: 2 → 0
#   4. use_activation_checkpointing: true (trades speed for ~50% less memory)

data:
  batch_size: 4                   # ⚠️ REDUCED from 32 (saves ~4x memory)
  num_workers: 4                   # ⚠️ REDUCED from 4 (each worker duplicates data)
  pin_memory: true

model:
  # --- Memory settings ---
  T_teacher: 7                     # ⚠️ REDUCED from 7 (max for 8 timesteps)
  T_rollout: 3                     # Fixed: predict 5 timesteps autoregressively <- changed to 3 just because of lack of memory VRAM
  context_frames: 3                # Fixed: use 3 ground-truth context frames
  use_activation_checkpointing: true  # ⚠️ ENABLED (trades compute for ~50% memory)

  # --- Model architecture (keep defaults from ac_predictor.yaml) ---
  # Uncomment to override:
  # num_timesteps: 8  # Encoded timesteps (default, matches precomputed features)
  # embed_dim: 1024
  # predictor_embed_dim: 384
  # depth: 24
  # num_heads: 16
  # mlp_ratio: 4.0

  # --- Action dimension (dataset-specific) ---
  action_embed_dim: 2              # 2D actions for this dataset

  # --- Loss weights ---
  loss_weight_teacher: 1.0
  loss_weight_rollout: 1.0

  # --- Curriculum Learning Schedule ---
  # Dynamically adjusts loss_weight_teacher during training.
  # T_rollout and context_frames are now FIXED (3 context → 5 predictions).
  # Each phase activates at the specified epoch and remains active until the next phase.
  # Parameters not specified in a phase retain their previous values.
  #
  # Recommended progression (from meine_eigene_Loss_Logik.md):
  #   Phase 1: Stabilization - high teacher weight
  #   Phase 2: Transition - reduce teacher weight
  #   Phase 3: Rollout-dominant - model learns long-horizon dynamics
  #
  # Set to null to disable curriculum learning and use fixed values above.
  curriculum_schedule:
    - epoch: 0
      loss_weight_teacher: 1.0
    - epoch: 5
      loss_weight_teacher: 0.7
    - epoch: 7
      loss_weight_teacher: 0.3

  # --- Optimizer (V-JEPA2 paper settings) ---
  learning_rate: 4.25e-4           # Peak LR
  weight_decay: 0.04
  betas: [0.9, 0.999]

  # --- LR schedule (iteration-based, V-JEPA2 paper) ---
  use_iteration_scheduler: true
  warmup_iters: 400
  constant_iters: 3895
  decay_iters: 400
  warmup_start_lr: 7.5e-5

# =============================================================================
# TRAINER SETTINGS
# =============================================================================
trainer:
  max_epochs: 15
  gradient_clip_val: 1.01
  gradient_clip_algorithm: norm
