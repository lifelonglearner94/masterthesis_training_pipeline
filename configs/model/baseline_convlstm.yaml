# @package _global_

# Baseline ConvLSTM Model Configuration
# Action-Conditioned ConvLSTM Baseline for comparison with ViT AC Predictor
#
# Architecture: ConvLSTM with spatial action tiling and residual learning
# - Encoder: 1×1 Conv (1024 → 256)
# - ConvLSTM: Temporal modeling with spatial convolutions
# - Decoder: 1×1 Conv (256 → 1024) with residual connection
#
# Reference: docs/possible_Baseline.md

defaults:
  - /model/_empty  # Override empty default

model:
  _target_: src.models.baseline_convlstm.BaselineLitModule

  # Model architecture
  input_dim: 1024         # V-JEPA2 feature dimension
  hidden_dim: 256         # ConvLSTM hidden dimension (bottleneck)
  action_dim: 2           # 2D actions for this dataset
  spatial_size: 16        # 16×16 spatial grid (256 patches)
  kernel_size: 3          # ConvLSTM kernel size
  num_timesteps: 8        # Encoded timesteps (must match data)

  # Loss settings (same as ac_predictor.yaml for fair comparison)
  T_teacher: 7            # Teacher-forcing prediction steps
  jump_k: 7            # Number of candidate jump targets
  loss_weight_teacher: 1.0
  loss_weight_jump: 1.0
  normalize_reps: true    # Apply layer norm after predictor steps (V-JEPA2 paper)
  loss_type: "l1"         # Loss function: "l1" (MAE), "l2" (MSE), or "huber"
  huber_delta: 1.0        # Delta for Huber loss (only used when loss_type="huber")

  # Curriculum learning schedule (null = disabled, use fixed values above)
  # Will be overridden in experiment config for fair comparison
  curriculum_schedule: null

  # Optimizer settings (same as ac_predictor.yaml)
  learning_rate: 4.25e-4
  weight_decay: 0.04
  betas: [0.9, 0.999]
  warmup_epochs: 10
  max_epochs: 100

  # LR schedule (iteration-based, same as ac_predictor.yaml)
  use_iteration_scheduler: false
  warmup_pct: 0.085
  constant_pct: 0.83
  decay_pct: 0.085
  warmup_start_lr: 7.5e-5
