# @package _global_

# AC-HOPE-ViT Model Configuration
# Action-Conditioned HOPE Vision Transformer (Novel Architecture)
#
# Combines V-JEPA2 AC predictor I/O pipeline with HOPE backbone:
#   - Self-Modifying Titan memories (replace attention)
#   - CMS multi-frequency MLPs (replace FFN)
#
# Same input/output format as ac_predictor.yaml:
#   Input:  [B, T*N, D=1024] + [B, T, action_dim]
#   Output: [B, T*N, D=1024]

defaults:
  - /model/_empty  # Override empty default

model:
  _target_: src.models.hope.ACHOPEModule

  # ─── Model architecture (Stage 1 & 3 — identical to AC predictor) ───
  img_size: [256, 256]
  patch_size: 16
  num_timesteps: 8  # Encoded timesteps (16 original frames / tubelet_size=2)
  embed_dim: 1024
  predictor_embed_dim: 384
  depth: 24
  num_heads: 16
  action_embed_dim: 7  # 7D: position (3), orientation (3), gripper (1)
  is_frame_causal: true
  use_activation_checkpointing: false
  use_extrinsics: false

  # ─── HOPE-specific architecture (Stage 2 — novel) ───

  # 3D RoPE toggle (Criticism §2: enable/disable for ablation)
  use_rope: true

  # Titan memory settings
  titan_hidden_multiplier: 4    # Titan MLP hidden dim = dim * multiplier
  titan_layers: 2               # Number of layers in Titan MLP
  titan_activation: "gelu"      # Activation function in Titan memory
  titan_grad_clip_inner: 1.0    # Gradient clipping for inner-loop (DGD)

  # CMS multi-frequency MLP levels
  # Each level has a name, update_period, and hidden_multiplier
  # update_period=1 means update every token (fastest)
  # update_period=16 means update every 16th token (slowest)
  cms_level_specs:
    - name: "fast"
      update_period: 1
      hidden_multiplier: 4.0
      warmup_steps: 0
    - name: "medium"
      update_period: 4
      hidden_multiplier: 4.0
      warmup_steps: 0
    - name: "slow"
      update_period: 16
      hidden_multiplier: 4.0
      warmup_steps: 0
  cms_use_chunk_scheduling: false  # Enable frequency-based chunk scheduling

  # Chunk-wise sequential memory updates (Section 8.2)
  # Number of timesteps per chunk. 0 = no chunking (all timesteps at once).
  # With chunking, M_eta and M_alpha get outer-loop gradients because
  # chunk 2 reads memory state modified in chunk 1.
  #
  # ⚠️ VRAM WARNING: chunk_size > 0 retains computation graph across chunks
  # (5 memories × T chunks × depth layers). This can easily OOM on consumer
  # GPUs / MPS. Use chunk_size=0 for limited VRAM.
  #
  # Values: 0 = safe default (trains M_memory only)
  #         1 = per-timestep (also trains M_eta, M_alpha; needs ~2-3× VRAM)
  chunk_size: 0

  # Detach memory graph every N update steps to bound VRAM (0 = never).
  # With chunk_size=0 this only affects the single DGD step per block.
  # With chunk_size>0, set to 1-2 for MPS/consumer GPU, 4+ for datacenter GPU.
  titan_detach_interval: 1

  surprise_threshold: 0.0       # Min surprise to trigger memory update (0=always)

  # Spatial mixing (Phase C) — adds cross-token interaction within each frame
  # Without this, HOPE processes each token independently (no spatial interaction)
  use_spatial_mixing: false

  # ─── Diagnostics (Criticism §1: log all relevant parameters) ───
  log_hope_diagnostics: true

  # ─── Regularization ───
  drop_rate: 0.0
  drop_path_rate: 0.1

  # ─── Loss settings (same as AC predictor for comparability) ───
  T_teacher: 7                   # Teacher-forcing prediction steps
  jump_k: 3                   # Number of candidate jump targets
  loss_weight_teacher: 1.0
  loss_weight_jump: 1.0
  normalize_reps: true            # Layer norm after predictor steps (V-JEPA2 paper)
  loss_type: "l1"                   # "l1" (MAE), "l2" (MSE), or "huber"
  huber_delta: 1.0                  # Delta for Huber loss (only used when loss_type="huber")

  # Curriculum learning (null = disabled)
  curriculum_schedule: null

  # ─── Optimizer settings ───
  learning_rate: 1e-4
  weight_decay: 0.05
  betas: [0.9, 0.999]
  warmup_epochs: 10
  max_epochs: ${trainer.max_epochs}

  # Per-group LR scaling (Titan memories get lower LR for stability)
  titan_lr_scale: 0.2             # Titan params LR = learning_rate * 0.2
  cms_lr_scale: 1.0               # CMS params LR = learning_rate * 1.0
  titan_weight_decay: 0.005       # Separate WD for Titan (avoids double reg with inner-loop α)
  aux_loss_weight: 0.1            # Weight for M_k/M_v auxiliary retrieval-quality loss
